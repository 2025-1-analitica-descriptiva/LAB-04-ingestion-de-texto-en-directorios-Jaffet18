{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a948e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura de directorios:\n",
      "['.git', '.github', '.gitignore', '.mypy_cache', '.pytest_cache', '.venv', '.vscode', 'Ayuda.ipynb', 'files', 'homework', 'homework.egg-info', 'README.md', 'requirements.txt', 'setup.bat', 'setup.py', 'setup.sh', 'tests']\n",
      "Advertencia: No se encontró train\\positive\n",
      "Advertencia: No se encontró train\\negative\n",
      "Advertencia: No se encontró train\\neutral\n",
      "No se encontraron datos para train\n",
      "Advertencia: No se encontró test\\positive\n",
      "Advertencia: No se encontró test\\negative\n",
      "Advertencia: No se encontró test\\neutral\n",
      "No se encontraron datos para test\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def pregunta_01():\n",
    "    \"\"\"\n",
    "    La información requerida para este laboratio esta almacenada en el\n",
    "    archivo \"files/input.zip\" ubicado en la carpeta raíz.\n",
    "    Descomprima este archivo.\n",
    "\n",
    "    Como resultado se creara la carpeta \"input\" en la raiz del\n",
    "    repositorio, la cual contiene la siguiente estructura de archivos:\n",
    "\n",
    "\n",
    "    ```\n",
    "    train/\n",
    "        negative/\n",
    "            0000.txt\n",
    "            0001.txt\n",
    "            ...\n",
    "        positive/\n",
    "            0000.txt\n",
    "            0001.txt\n",
    "            ...\n",
    "        neutral/\n",
    "            0000.txt\n",
    "            0001.txt\n",
    "            ...\n",
    "    test/\n",
    "        negative/\n",
    "            0000.txt\n",
    "            0001.txt\n",
    "            ...\n",
    "        positive/\n",
    "            0000.txt\n",
    "            0001.txt\n",
    "            ...\n",
    "        neutral/\n",
    "            0000.txt\n",
    "            0001.txt\n",
    "            ...\n",
    "    ```\n",
    "\n",
    "    A partir de esta informacion escriba el código que permita generar\n",
    "    dos archivos llamados \"train_dataset.csv\" y \"test_dataset.csv\". Estos\n",
    "    archivos deben estar ubicados en la carpeta \"output\" ubicada en la raiz\n",
    "    del repositorio.\n",
    "\n",
    "    Estos archivos deben tener la siguiente estructura:\n",
    "\n",
    "    * phrase: Texto de la frase. hay una frase por cada archivo de texto.\n",
    "    * sentiment: Sentimiento de la frase. Puede ser \"positive\", \"negative\"\n",
    "      o \"neutral\". Este corresponde al nombre del directorio donde se\n",
    "      encuentra ubicado el archivo.\n",
    "\n",
    "    Cada archivo tendria una estructura similar a la siguiente:\n",
    "\n",
    "    ```\n",
    "    |    | phrase                                                                                                                                                                 | target   |\n",
    "    |---:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------|\n",
    "    |  0 | Cardona slowed her vehicle , turned around and returned to the intersection , where she called 911                                                                     | neutral  |\n",
    "    |  1 | Market data and analytics are derived from primary and secondary research                                                                                              | neutral  |\n",
    "    |  2 | Exel is headquartered in Mantyharju in Finland                                                                                                                         | neutral  |\n",
    "    |  3 | Both operating profit and net sales for the three-month period increased , respectively from EUR16 .0 m and EUR139m , as compared to the corresponding quarter in 2006 | positive |\n",
    "    |  4 | Tampere Science Parks is a Finnish company that owns , leases and builds office properties and it specialises in facilities for technology-oriented businesses         | neutral  |\n",
    "    ```\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear la carpeta output si no existe\n",
    "    print(\"Estructura de directorios:\")\n",
    "    print(os.listdir('.'))\n",
    "    if os.path.exists('train'):\n",
    "        print(\"Contenido de train:\", os.listdir('train'))\n",
    "    if os.path.exists('test'):\n",
    "        print(\"Contenido de test:\", os.listdir('test'))\n",
    "\n",
    "\n",
    "    # Asegurarnos que existe el directorio de salida\n",
    "    os.makedirs('files/output', exist_ok=True)\n",
    "    \n",
    "    # Procesar tanto train como test\n",
    "    for dataset in ['train', 'test']:\n",
    "        data = []\n",
    "        \n",
    "        # Procesar cada categoría de sentimiento\n",
    "        for sentiment in ['positive', 'negative', 'neutral']:\n",
    "            folder_path = os.path.join(dataset, sentiment)\n",
    "            \n",
    "            # Verificar si la carpeta existe\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"Advertencia: No se encontró {folder_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Leer todos los archivos .txt en la carpeta\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith('.txt'):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    \n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read().strip()\n",
    "                            data.append({\n",
    "                                'phrase': content,\n",
    "                                'target': sentiment\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error leyendo {file_path}: {str(e)}\")\n",
    "        \n",
    "        # Crear DataFrame y guardar como CSV\n",
    "        if data:  # Solo si hay datos\n",
    "            df = pd.DataFrame(data)\n",
    "            output_path = f'files/output/{dataset}_dataset.csv'\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"Archivo {output_path} generado con {len(df)} registros\")\n",
    "        else:\n",
    "            print(f\"No se encontraron datos para {dataset}\")\n",
    "    \n",
    "    return\n",
    "\n",
    "print(pregunta_01())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871429a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando estructura de directorios...\n",
      "Contenido actual: ['.git', '.github', '.gitignore', '.mypy_cache', '.pytest_cache', '.venv', '.vscode', 'Ayuda.ipynb', 'files', 'homework', 'homework.egg-info', 'README.md', 'requirements.txt', 'setup.bat', 'setup.py', 'setup.sh', 'tests']\n",
      "\n",
      "¡Atención! No se encontró train\\positive\n",
      "\n",
      "¡Atención! No se encontró train\\negative\n",
      "\n",
      "¡Atención! No se encontró train\\neutral\n",
      "\n",
      "❌ No se encontraron datos para train\n",
      "\n",
      "¡Atención! No se encontró test\\positive\n",
      "\n",
      "¡Atención! No se encontró test\\negative\n",
      "\n",
      "¡Atención! No se encontró test\\neutral\n",
      "\n",
      "❌ No se encontraron datos para test\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def pregunta_01():\n",
    "    \"\"\"\n",
    "    Procesa archivos de texto en carpetas train/test y genera CSVs con las frases y sentimientos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verificar estructura de directorios (solo para diagnóstico)\n",
    "    print(\"\\nVerificando estructura de directorios...\")\n",
    "    print(\"Contenido actual:\", os.listdir('.'))\n",
    "    if os.path.exists('train'):\n",
    "        print(\"\\nContenido de train/:\", os.listdir('train'))\n",
    "        if os.path.exists('train/positive'):\n",
    "            print(\"Archivos en train/positive:\", len(os.listdir('train/positive')))\n",
    "        if os.path.exists('train/negative'):\n",
    "            print(\"Archivos en train/negative:\", len(os.listdir('train/negative')))\n",
    "        if os.path.exists('train/neutral'):\n",
    "            print(\"Archivos en train/neutral:\", len(os.listdir('train/neutral')))\n",
    "    \n",
    "    if os.path.exists('test'):\n",
    "        print(\"\\nContenido de test/:\", os.listdir('test'))\n",
    "        if os.path.exists('test/positive'):\n",
    "            print(\"Archivos en test/positive:\", len(os.listdir('test/positive')))\n",
    "        if os.path.exists('test/negative'):\n",
    "            print(\"Archivos en test/negative:\", len(os.listdir('test/negative')))\n",
    "        if os.path.exists('test/neutral'):\n",
    "            print(\"Archivos en test/neutral:\", len(os.listdir('test/neutral')))\n",
    "\n",
    "    # Crear carpeta output si no existe\n",
    "    os.makedirs('files/output', exist_ok=True)\n",
    "\n",
    "    # Procesar train y test\n",
    "    for dataset in ['train', 'test']:\n",
    "        data = []\n",
    "        \n",
    "        for sentiment in ['positive', 'negative', 'neutral']:\n",
    "            folder_path = os.path.join(dataset, sentiment)\n",
    "            \n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"\\n¡Atención! No se encontró {folder_path}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nProcesando {folder_path}...\")\n",
    "            \n",
    "            for filename in sorted(os.listdir(folder_path)):\n",
    "                if filename.endswith('.txt'):\n",
    "                    filepath = os.path.join(folder_path, filename)\n",
    "                    \n",
    "                    try:\n",
    "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                            phrase = f.read().strip()\n",
    "                            data.append({\n",
    "                                'phrase': phrase,\n",
    "                                'target': sentiment  # Usamos 'target' como pide el test\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error al leer {filepath}: {str(e)}\")\n",
    "                        continue\n",
    "        \n",
    "        # Crear DataFrame y guardar CSV\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            output_path = f'files/output/{dataset}_dataset.csv'\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"\\n✅ Archivo {output_path} creado con éxito\")\n",
    "            print(f\"Registros totales: {len(df)}\")\n",
    "            print(\"Muestra de datos:\")\n",
    "            print(df.head(3))\n",
    "        else:\n",
    "            print(f\"\\n❌ No se encontraron datos para {dataset}\")\n",
    "    \n",
    "    return\n",
    "\n",
    "print(pregunta_01())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79fa5f4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ejecuta esto en tu entorno\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfiles/output/train_dataset.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrain dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_df[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m].value_counts())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-04-ingestion-de-texto-en-directorios-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-04-ingestion-de-texto-en-directorios-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-04-ingestion-de-texto-en-directorios-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-04-ingestion-de-texto-en-directorios-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub\\LABORATORIOS\\LAB-04-ingestion-de-texto-en-directorios-Jaffet18\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# Ejecuta esto en tu entorno\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('files/output/train_dataset.csv')\n",
    "print(f\"\\nTrain dataset shape: {train_df.shape}\")\n",
    "print(train_df['target'].value_counts())\n",
    "\n",
    "test_df = pd.read_csv('files/output/test_dataset.csv')\n",
    "print(f\"\\nTest dataset shape: {test_df.shape}\")\n",
    "print(test_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c44556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando estructura...\n",
      "Contenido de files/input: ['test', 'train']\n",
      "\n",
      "Procesando files/input\\train...\n",
      "Leyendo archivos en files/input\\train\\positive...\n",
      "Leyendo archivos en files/input\\train\\negative...\n",
      "Leyendo archivos en files/input\\train\\neutral...\n",
      "✅ files/output\\train_dataset.csv creado con 1811 registros\n",
      "Muestra de datos:\n",
      "                                              phrase    target\n",
      "0  Both operating profit and net sales for the th...  positive\n",
      "1  The company 's scheduled traffic , measured in...  positive\n",
      "\n",
      "Procesando files/input\\test...\n",
      "Leyendo archivos en files/input\\test\\positive...\n",
      "Leyendo archivos en files/input\\test\\negative...\n",
      "Leyendo archivos en files/input\\test\\neutral...\n",
      "✅ files/output\\test_dataset.csv creado con 453 registros\n",
      "Muestra de datos:\n",
      "                                              phrase    target\n",
      "0  Operating profit rose to EUR 13.1 mn from EUR ...  positive\n",
      "1  Consolidated net sales increased 16 % to reach...  positive\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def pregunta_01():\n",
    "    # 1. Configurar rutas correctas\n",
    "    input_base = 'files/input'\n",
    "    output_dir = 'files/output'\n",
    "    \n",
    "    # 2. Verificar estructura\n",
    "    print(\"\\nVerificando estructura...\")\n",
    "    print(\"Contenido de files/input:\", os.listdir(input_base))\n",
    "    \n",
    "    # 3. Crear directorio de salida\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 4. Procesar train y test\n",
    "    for dataset in ['train', 'test']:\n",
    "        data = []\n",
    "        dataset_path = os.path.join(input_base, dataset)\n",
    "        \n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"\\n¡Error! No existe {dataset_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcesando {dataset_path}...\")\n",
    "        \n",
    "        for sentiment in ['positive', 'negative', 'neutral']:\n",
    "            sentiment_path = os.path.join(dataset_path, sentiment)\n",
    "            \n",
    "            if not os.path.exists(sentiment_path):\n",
    "                print(f\"¡Atención! No se encontró {sentiment_path}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Leyendo archivos en {sentiment_path}...\")\n",
    "            \n",
    "            for filename in sorted(os.listdir(sentiment_path)):\n",
    "                if filename.endswith('.txt'):\n",
    "                    filepath = os.path.join(sentiment_path, filename)\n",
    "                    \n",
    "                    try:\n",
    "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                            phrase = f.read().strip()\n",
    "                            data.append({\n",
    "                                'phrase': phrase,\n",
    "                                'target': sentiment\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error leyendo {filepath}: {str(e)}\")\n",
    "        \n",
    "        # Guardar CSV\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            output_path = os.path.join(output_dir, f'{dataset}_dataset.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"✅ {output_path} creado con {len(df)} registros\")\n",
    "            print(\"Muestra de datos:\")\n",
    "            print(df.head(2))\n",
    "        else:\n",
    "            print(f\"❌ No se procesaron datos para {dataset}\")\n",
    "    \n",
    "    return\n",
    "\n",
    "print(pregunta_01())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8206b3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train dataset shape: (1811, 2)\n",
      "target\n",
      "neutral     1117\n",
      "positive     458\n",
      "negative     236\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test dataset shape: (453, 2)\n",
      "target\n",
      "neutral     274\n",
      "positive    112\n",
      "negative     67\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta esto en tu entorno\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('files/output/train_dataset.csv')\n",
    "print(f\"\\nTrain dataset shape: {train_df.shape}\")\n",
    "print(train_df['target'].value_counts())\n",
    "\n",
    "test_df = pd.read_csv('files/output/test_dataset.csv')\n",
    "print(f\"\\nTest dataset shape: {test_df.shape}\")\n",
    "print(test_df['target'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
